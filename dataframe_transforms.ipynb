{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "043ac59f-996e-47c5-bf9b-35e32fb346e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8ac629-d249-4ac0-b781-f47d88979ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_dataframe(filepath: str):\n",
    "    '''\n",
    "    Loads in json objects from S3 bucket into dataframe:\n",
    "\n",
    "    Params: filepath -> Location of topic within S3 Bucket\n",
    "    Returns: df -> Dataframe of combined json objects for topic\n",
    "    '''\n",
    "    file_type = \"json\"\n",
    "    infer_schema = \"true\"\n",
    "    df = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .load(filepath)\n",
    "    return df\n",
    "\n",
    "pin = load_dataframe('/mnt/paaaaaat/topics/12baff1ff207.pin/partition=0/*.json')\n",
    "geo = load_dataframe('/mnt/paaaaaat/topics/12baff1ff207.geo/partition=0/*.json')\n",
    "user = load_dataframe('/mnt/paaaaaat/topics/12baff1ff207.user/partition=0/*.json')\n",
    "# display(pin)\n",
    "print(type(pin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90bb436-3b77-482f-a181-4a282c478105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(type(pin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba72b0df-ec9f-4405-8337-687d22c27be6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "desc: No description available\n",
    "desc: No description available Story format\n",
    "follower_count, poster_name: User Info Error\n",
    "tag_list: N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\n",
    "title: No Title Data Available\n",
    "image_src: Image src error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8666b1e1-887f-433c-bc79-ebbc292f75ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_pin_df(pin_df: DataFrame):\n",
    "    misc_rows = ['No description available', 'No description available Story format', 'User Info Error'\n",
    "                 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e', 'No Title Data Available', 'Image src error.']\n",
    "    print(type(pin_df))\n",
    "    for col_ite in pin_df.columns:\n",
    "        pin_df = pin_df.withColumn(col_ite, when(col(col_ite).isin(misc_rows), None).otherwise(col(col_ite)))\n",
    "    display(pin_df)\n",
    "\n",
    "    pin_df = pin_df.withColumn('follower_count',\n",
    "                               when(col('follower_count').contains('k'), (regexp_replace(col('follower_count'), 'k', '000'))\\\n",
    "                                        .cast('int'))\\\n",
    "                                        .when(col('follower_count').contains('M'), (regexp_replace(col('follower_count'), 'M', '000000'))\\\n",
    "                                            .cast('int'))\\\n",
    "                                            .otherwise(col(\"follower_count\").cast(\"int\")))\n",
    "    # Ensure numeric data type for columns below\n",
    "    pin_df = pin_df.withColumn(\"downloaded\", col(\"downloaded\").cast(\"int\"))\n",
    "    pin_df = pin_df.withColumn(\"index\", col(\"index\").cast(\"int\"))\n",
    "\n",
    "    # Include only location path\n",
    "    pin_df = pin_df.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), \"Local save in \", \"\"))\n",
    "\n",
    "    # Rename the index column to ind\n",
    "    pin_df = pin_df.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    column_order = [\n",
    "        \"ind\",\n",
    "        \"unique_id\",\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        \"follower_count\",\n",
    "        \"poster_name\",\n",
    "        \"tag_list\",\n",
    "        \"is_image_or_video\",\n",
    "        \"image_src\",\n",
    "        \"save_location\",\n",
    "        \"category\"\n",
    "    ]\n",
    "    pin_df = pin_df.select(column_order)\n",
    "    # display(pin_df)\n",
    "    print(type(pin_df))\n",
    "    return pin_df\n",
    "\n",
    "pin_clean = clean_pin_df(pin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a1d9d1-938b-4d38-a269-25c7ce2633c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_df_geo(df_geo: DataFrame) -> DataFrame:\n",
    "    # Create new column with values from latitude and longitude in array\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "    df_geo = df_geo.drop(\"latitude\",\"longitude\")\n",
    "\n",
    "    df_geo = df_geo.withColumn(\"timestamp\",to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "    df_geo = df_geo.withColumn(\"ind\", col(\"ind\").cast(\"int\"))\n",
    "\n",
    "    column_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "    df_geo = df_geo.select(column_order)\n",
    "    display(df_geo)\n",
    "    print(type(df_geo))\n",
    "    return df_geo\n",
    "\n",
    "geo_clean = clean_df_geo(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735d1856-fa76-4889-bbf5-e5e9f518582e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_df_user(df_user: DataFrame) -> DataFrame:\n",
    "\n",
    "    df_user = df_user.withColumn(\"user_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "    df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "    df_user = df_user.withColumn(\"date_joined\", to_timestamp(col(\"date_joined\")))\n",
    "\n",
    "    df_user = df_user.withColumn(\"ind\", col(\"ind\").cast(\"int\"))\n",
    "    df_user = df_user.withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "\n",
    "    column_order = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "    df_user = df_user.select(column_order)\n",
    "    display(df_user)\n",
    "    print(type(df_user))\n",
    "    return df_user\n",
    "\n",
    "user_clean = clean_df_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3b049b-2e44-4213-82c4-cf7cb448a01b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pin_clean.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"pin_clean\")\n",
    "geo_clean.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"geo_clean\")\n",
    "user_clean.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"user_clean\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataframe_transforms",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
